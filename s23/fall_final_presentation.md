---
layout: f22
title: Fall Capstone Final Presentation Information
nav_order: 7
---

# Fall Capstone Final Presentation Schedule and Information

<html>
<style>
table,tr {
 
table-layout: fixed ;
 width: 100% ;
}

img {

width: 100%;
height: auto;
overflow: hidden;
position: relative;

}
td {
vertical-align: top;
}

.center {
 display: block;
 margin-left: auto;
 margin-right: auto;
 
}
p {
text-align: center; 
}
h2 {
vertical-align: top;
text-align: center;
padding-top: 25px;
padding-bottom: 25px;
}
</style>
<div>
<div>
<table>
<tbody>
<tr style="text-align: center;">
<td>
<h2>AI Guide Dog: Predicting Egocentric Movement on a Smartphone</h2>
<img src="https://i.postimg.cc/qqTFCqBf/Screenshot-2022-11-23-at-22-52-41-Abhishree-Shetty.png" alt="Shetty" />
<p>Abhishree Shetty, Aishwarya Jadhav, Jeffery Cao, Aditi Sharma, Ben Sukboontip, Jayant Sravan Tamarapalli, Jingyi Zhang, Urvashi Priyam Kumar</p>
<p>AI Guide Dog attempts to democratize egocentric trajectory prediction on a smartphone for the blind community. We propose an end-to-end data pipeline for self-supervised egocentric trajectory prediction research consisting of the following: (1) a data collection module ingesting data signals generally accessible through default sensors and software stack that ship with popular smartphones; (2) a data annotation module built on top of collected data that requires minimal human tuning: (3) an egocentric modeling module to provide future directions to the users.We also demo an iOS application that takes advantage of the learned models to make real-time predictions. We will release our dataset and models post-publication.<br /><br /> Monday, December 5<br /> TEP 1403<br /> 10:10-11:30<br /> <a href="https://youtu.be/xj0R7URP5pg"> Presentation Recording</a></p>
</td>
<td>
<h2>Komatsu: Intelligent Longwalls</h2>
<img src="https://i.postimg.cc/3JsgLYZb/komatsu-Anveshrithaa-Sundareswaran.png" alt="Komatsu" />
<p>Anveshrithaa Sundareswaran, Ge Huang, Zhiyi Li, Zhouyang Li</p>
<p>Komatsu is a leading manufacturer of mining equipment. Komatsu uses intelligent longwall systems that capture machine data from automated, connected machinery and transform it into useful decision-making knowledge to support decision-making in machine operation and maintenance. This project aims to apply machine learning to build and deploy pipelines for identifying correlations, patterns, and anomalies in longwall machines to improve the operational efficiency of the machines and reduce energy consumption.<br /><br /> Monday, December 5<br /> GHC 4405<br /> 1:00-1:40<br /> <a href="https://youtu.be/9tw98inFvRY"> Presentation Recording</a></p>
</td>
<td>
<h2>SimBot - Navigation and Interaction</h2>
<img src="https://i.postimg.cc/d164xBm1/Simbot-Balashowry-Vatti.jpg" alt="Vatti" />
<p>Adhokshaja Madhwaraj, Jessica Zhong, Kushagra Mahajan, Malaika Vijay, Sai Vishwas Padigi, Vineeth Reddy Vatti</p>
<p>Embodied task-completion agents are intelligent agents that can perceive, navigate, and manipulate objects in an environment. We developed a bot for the SimBot challenge in conversational embodied task completion, where users can converse with and guide the bot to complete tasks in a 3D environment. The Navigation and Interaction thrust of the SimBot project focuses on scene understanding and action planning to perform actions that satisfy a user&rsquo;s instruction. Our primary contributions include an Image Segmentation model to identify objects of interest and an Action Planning module capable of generating a logical sequence of actions to execute on the simulator.<br /><br /> Tuesday, December 6<br /> POS 153 <br /> 8:35-9:15<br /> <a href="https://youtu.be/mqhVzn91CgA"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>SimBot Dataset Collection: An Embodied Vision-Audio-Navigation Task</h2>
<img src="https://i.postimg.cc/N0rS3sh9/Data-Collection-Workflow-Linyi-Li.png" alt="Linyi" />
<p>Bharani Ujjaini Kempaiah, Linyi Li</p>
<p>Intelligent robotic agents that operate in human spaces must be capable of realizing and executing instructions that are conveyed in natural language via speech. Most existing vision and language benchmarks contain instructions in natural language via text, and subsequently, models trained on these datasets also rely heavily on text as an intermediate medium to achieve grounding. In this work, we aim to enhance the existing ALFRED benchmark by crowd-sourcing speech annotations for existing ALFRED demonstrations of household tasks. We build web interfaces to qualify crowd workers before collecting speech annotations online. With these speech annotations, we believe the research community can develop models that can power robots to achieve audio-visual grounding similar to how humans interact in the real world.<br /><br /> Tuesday, December 6<br /> POS 153<br /> 9:15-9:55<br /> <a href="https://youtu.be/Fs4plsjD8yU"> Presentation Recording</a></p>
</td>
<td>
<h2>Learn-to-Race: A Multimodal Control Environment for Autonomous Racing</h2>
<img src="https://i.postimg.cc/W1Nxpdzx/simulator-ss-Tanay-Gangey.jpg" alt="Tanay" />
<p>Kevin Chian, Arav Agarwal, Sidharth Kathpal, Yujun Qin, Tanay Gangey</p>
<p>Autonomous racing is a sub-field of autonomous driving which has been studied considerably less than urban driving. To further research in this area, we implement a set of extensions spanning reinforcement learning (RL), Computer Vision (CV), and robotics interfaces upon the Learn-to-Race framework, which itself runs on top of a racing simulator. Our project also has a software development component that involves creating interfaces to connect to an actual vehicle using ROS. These research and development thrusts are crucial to designing safe and fast autonomous agents, as failures in real-life are exceptionally costly. The safe policies learned by the autonomous racers can be generalized to the real world to have safer and faster autonomous driving agents.<br /><br /> Tuesday, December 6<br /> POS 153 <br /> 11:50-1:10<br /> <a href= "https://youtu.be/5DRM5kHrAEc"> Presentation Recording</a></p>
</td>
<td>
<h2>EnvPool</h2>
<img src="https://i.postimg.cc/0NRgGSmb/Screenshot-2022-11-22-at-11-03-41-Yukun-Jiang.png" alt="Yukun" width="768" height="314" />
<p>Yukun Jiang, Leo Guo, Yufan Song, Ting Luo, Tianyi Sun, Peilin Rao,<br /><br /> EnvPool is a C++-based batched environment pool with pybind11 and thread pool. It has high performance (~1M raw FPS with Atari games, ~3M raw FPS with Mujoco simulator on DGX-A100) and compatible APIs (supports both gym and dm_env, both sync and async, both single and multiplayer environment)<br /><br /> Wednesday, December 7<br /> TEP 1403<br /> 10:10-10:50 <br /> <a href= ""> Presentation Recording Not Available</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Multimodal Question Answering</h2>
<img src="https://i.postimg.cc/DwSzHXx2/5-Manoj-Ghuhan-Arivazhagan.png" alt="Ghuhan" />
<p>Kunal Dhawan, Manoj Ghuhan Arivazhagan, Wenxing Deng</p>
<p> Multimodal Question Answering (MQA) is a rapidly growing area of research that aims at building intelligent systems that can respond to user queries by reasoning over information from multiple modalities. Such systems try to emulate human beings who also rely on cross-modal reasoning to answer any question thrown at them. Current MQA approaches suffer from various drawbacks like biased datasets used for training, the inability to answer simple counting-based questions, and the tendency to learn surface-level relationships rather than building reasoning. In this work, we aim to overcome these limitations and propose a new end-to-end MQA system. The major contributions of this work would be: 1) Curation of an MQA dataset which consists of a diverse set of question types capturing complex interactions and relationships between different objects in the images and is devoid of any inherent biases, 2) Improved feature extraction module which can handle and even generate scene graphs given input images, 3) Instance segmentation module to improve MQA system performance for counting related questions, 4) End-to-end trainable MQA pipeline which outperforms the current state-of-the-art.<br /><br /> Wednesday, December 7<br /> TEP 1403<br /> 10:50-11:30<br /> <a href="https://youtu.be/itVnyh9oEHQ"> Presentation Recording</a></p>
</td>
<td>
<h2>ASML YieldStar: Particle / Defect Detection and Classification</h2>
<img src="https://i.postimg.cc/hjN9HnBj/Screen-Shot-2022-11-19-at-8-42-22-PM-Yijia-Zhang.png" alt="Yijia" />
<p>Mahalakshumi Visvanathan, Wei-Chieh Chen, Yijia Zhang</p>
<p>YieldStar is an advanced wafer metrology technique provided by ASML. It is used to verify the quality of the produced wafer. However, even tiny errors could affect production since YieldStar has complicated and expensive optics. Currently, production technicians rely on their knowledge to catch defects. This process takes over 500 hours per year, which is very time-consuming. To improve production efficiency, we've proposed two major techniques to improve it: a classification Convolutional Neural Network (CNN), which can detect if a particle is present or not, and a Convolutional Auto-Encoder that can localize and measure the intensity of the particle.<br /><br /> Wednesday, December 7<br /> GHC 4405 <br /> 1:00-1:40<br /> <a href="https://youtu.be/O5FtrPZTPxg"> Presentation Recording</a></p>
</td>
<td>
<h2>Predicting Flatness Error with ASML Wafer Table</h2>
<img src="https://i.postimg.cc/nhy4MZ5t/rep-image-Tz-Ruei-Liu.jpg" alt="Tz-Ruei-Liu" />
<p>Tz-Ruei Liu, I-Tsun Cheng, Xinyan Xie</p>
<p>At ASML, the flatness error of wafer tables is computed manually using a fixed formula. Although exact, the current system takes more than 5 minutes to compute all spec maps per sample, which is inefficient. In this work, we introduce a machine-learning method that predicts spec maps with sufficiently small errors while being substantially faster. We use U-Net and its variant SmaAt-UNet and show that they perform very effectively at our task. Equipped with attention and depthwise-separable convolutions, SmaAt-UNet achieves less than 2% error across all spec maps while taking only 30 seconds to compute, 10x faster than the current system.<br /><br /> Wednesday, December 7<br /> GHC 4405<br /> 1:40-2:20<br /> <a href="https://youtu.be/xquFhRnxd7c"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Self Driving Databases Management System</h2>
<img src="https://i.postimg.cc/NfFBbC7K/Screenshot-2022-11-28-at-11-04-06-PM-He-Wei-Lee.png" alt="Wei-Lee" />
<p>Kushagra Singh, Lichen Jin, He-Wei Lee</p>
<p>Our project presents the control plane for self-driving database systems -- a framework that orchestrates database tuning operations and manages resources for production database clusters hosted in heterogeneous environments.<br /><br /> Wednesday, December 7<br /> GHC 4405<br /> 2:20-3:00<br /> <a href= "https://youtu.be/F4_R-ib_H2U"> Presentation Recording</a></p>
</td>
<td>
<h2>Autobatch - Ragged Tensor's Shape Representation and Efficient Computation</h2>
<img src="https://i.postimg.cc/fL1jFrSM/autobatch-overview-Bowen-Chen.png" alt="Chen" />
<p>Bowen Chen</p>
<p>In this report, we will introduce a new shape representation of the Ragged Tensor, which is a typical input workload in NLP models. We will discuss: the design of Ragged Tensor Intermediate Representation (IR), the implementation of Ragged Tensor API upon Relax, a graph level optimization based on RaggedTensor IR, and an auto-batch user interface enabled by Ragged Tensor IR.<br /><br /> Wednesday, December 7<br /> GHC 4405<br /> 3:00-3:40<br /> <a href= "https://youtu.be/iAMo87tRgUA"> Presentation Recording</a></p>
</td>
<td>
<h2>Secure NLP Inference</h2>
<img src="https://i.postimg.cc/HxzpxqHt/Screenshot-2022-11-23-at-10-54-04-PM-Shreya-Sharma.png" alt="Sharma" />
<p>Shreya Sharma</p>
<p>With the advent of cloud computing benefits such as elasticity and availability at affordable rates, a large number of machine learning workloads are migrating to the cloud for operations. However, in this paradigm, sensitive data may be leaked to service providers if they are curious or compromised. This project aims at making existing NLP models oblivious, i.e., enabling secure inference on a trained BERT model without revealing any information about the client input data. Such a service can find uses in audits or private contract reviews, and entity recognition.<br /><br /> Thursday, December 8<br /> POS 153 <br /> 8:35-9:15<br /> <a href="https://youtu.be/jnu_cP0SFq8"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Data Augmentation for Information Retrieval</h2>
<img src="https://i.postimg.cc/PrQwMYPN/da-poster-picture-Riddhi-Nisar.png" alt="Riddhi" />
<p>Preksha Patel, Ramya Ramanathan, Riddhi Nisar, Sayani Kundu, Vivek Sourabh</p>
<p>In our capstone, we aim to build a data augmentation module, which can be easily plugged into the pipeline of Information Retrieval frameworks. We also aim to finetune and improve the re-ranker in FlexNeuART using augmented data. We use the MS MARCO passage re-ranking dataset for this task and implement three categories of data augmentation techniques - rule-based, model-based, and query reformulation. We evaluate our results on the mean reciprocal rank (MRR) metric.<br /><br /> Thursday, December 8<br /> POS 153<br /> 9:15-9:55<br /> <a href= "https://youtu.be/GPrnSnHZghw"> Presentation Recording</a></p>
</td>
<td>
<h2>Simbot Dialog and Language Generation</h2>
<img src="https://i.postimg.cc/FR5ZR6G1/minimap-Shubham-Phal.png" alt="Phal" />
<p>Shubham Phal, Nikhil Gupta, Prasoon Varshney, Shubham Virmani, Benny Jiang, Xinyue Chen</p>
<p>Our work on the Simbot Challenge asks a basic question: How does language change when situated? How do objects in the environment and behaviors of people around us inform how language utterances are interpreted? Our project uses advanced NLP techniques to model a two-way free-form dialogue between a commander and a multimodal embodied agent to perform a plethora of complex tasks in a simulated environment.<br /><br /> Thursday, December 8<br /> POS 153 <br /> 11:50-1:10<br /> <a href= "https://youtu.be/YlXZebG4y7E"> Presentation Recording</a></p>
</td>
<td>
<h2>Programmable Storage</h2>
<img src="https://i.postimg.cc/2y1y4Lb9/system-diagram-Jiuzhi-Yu.png" alt="Jiuzhi" width="500" height="400" />
<p>Jiuzhi Yu, Sumanth Rao</p>
<p>With more data to analyze in this big data era, traditional CPU-centric processing is constrained by the large volume of data transferred between the storage system and memory. In the meantime, with the advances in the computational storage devices in the industry, we are seeing the potential of offloading computation to a storage device to optimize the applications which process large amounts of data.In this project, we explore the potential of offloading RocksDB compaction operations and demonstrate the performance gain for the operation itself and for the whole system.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 10:00-10:40<br /> <a href="https://youtu.be/lPb9bJuscrc"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Automated Sensemaking for Online Tasks</h2>
<img src="https://i.postimg.cc/QCcGGjRx/highlevel2-Ye-Rin-Han.png" alt="Ye-Rin-Han" width="500" height="250" />
<p>Shuyu Jiang, Ye Rin Han</p>
<p>Skeema is an extension of the Google Chrome browser where users can manually organize their open tabs and nest them under user-defined tasks. We aim to enable Skeema to automatically categorize users' open tabs into tasks. We generate multiple features from the tab URL and title and build a Multi-layer Feed-forward Neural Network to experiment with different combinations of features. Using the distance matrix generated from the classifier, we create clusters of tabs through Agglomerative Clustering. The final clustering model achieves 0.8389 accuracy, and this result demonstrates that our approach to engineering the new features and model architecture is promising.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 10:40-11:20<br /> <a href="https://youtu.be/V2rR7EFXRNA"> Presentation Recording</a></p>
</td>
<td>
<h2>Intelligent Text-Conditioned Music Generation</h2>
<img src="https://i.postimg.cc/hv1x5dPD/System-design-diagram-Zhouyao-Xie.png" alt="Zhouyao" />
<p>Zhouyao Xie, Nikhil Yadala, Xinyi Chen, Jingxi Liu</p>
<p>Despite recent advancements in neural generative models and multimodal machine learning, the task of conditional music generation remains a niche research area that is largely under-explored. Inspired by CLIP, which learns to align image and text modalities through contrastive learning, we propose MusicCLIP, a text-conditioned music generation model with an encoder-decoder architecture. In the encoder part, the Transformer-based music encoder and text encoder learn to align each other through contrastive learning. In the decoder part, a music decoder generates symbolic music from latent embeddings using nucleus sampling.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 11:20-12:00<br /> <a href="https://youtu.be/ESgGjAORXmc"> Presentation Recording</a></p>
</td>
<td>
<h2>Accelerated Cloud for Artificial Intelligence - Systems</h2>
<img src="https://i.postimg.cc/k5cY3B15/ACAI-combined-Eeshwar-Gurushankar-Prasad.png" alt="Prasad" />
<p>Hao Yang Lu, Eeshwar Gurushankar Prasad, Shantanu Kamath, Chenda Zhang</p>
<p>ACAI helps ML practitioners focus on model development by providing utilities that reduce the development and deployment overheads that come with cloud infrastructure. We have implemented three novel features into the existing ACAI framework.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 1:00-1:40<br /> <a href= "https://youtu.be/6LqBF0onLSg"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Stargate: Towards DynamoDB Compatibility for Cassandra</h2>
<img src="https://i.postimg.cc/y8NgmTWj/deployment-model-Boxuan-Li.png" alt="Li" />
<p>Boxuan Li, Xiang Yue, Ziyan Zhang</p>
<p>We aim to develop a middleware that can bring DynamoDB API compatibility to Apache Cassandra so that DynamoDB applications can switch to Apache Cassandra seamlessly.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 1:40-2:20<br /> <a href= "https://youtu.be/qRZIDypYog0"> Presentation Recording</a></p>
</td>
<td>
<h2>End-to-end Model Probing</h2>
<img src="https://i.postimg.cc/xTHH7B8x/End-to-End-Model-Probing-Yunxuan-Xiao.png" alt="Yunxuan" width="500" height="400" />
<p>Yunxuan Xiao, Ashley Wu, Su Park, Divija Nagaraju</p>
<p>We introduce a theoretical and empirical framework for end-to-end model probing, which probes models for their ability to capture certain linguistic phenomena. We also discovered potential correlations between complex semantic tasks and low-level tasks from the probing results.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 2:20-3:00<br /> <a href="https://youtu.be/yacZT_bWOQw"> Presentation Recording</a></p>
</td>
<td>
<h2>Data Pipeline Wind Tunnel</h2>
<img src="https://i.postimg.cc/ZRjzx6WP/Screen-Shot-2022-11-28-at-10-06-38-PM-Shicheng-Huang.png" alt="Huang" />
<p>Shicheng Huang</p>
<p>Data pipeline wind tunnel is an end-to-end benchmarking system to allow stakeholders to evaluate the pipeline performance and make iterative modifications based on different configurations.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 3:00-3:40<br /> <a href="https://youtu.be/zOKz32BEZsY"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Accelerated Cloud for Artificial Intelligence - AutoML</h2>
<img src="https://i.postimg.cc/rsnHb0HX/systems-architecture-Ke-Sun.png" alt="Ke-Sun" />
<p>Jiyu Hu, Ruben Mampilli, Ke Sun</p>
<p>ACAI AutoML provides a solution to machine learning tuning through automating machine learning experimentation based on a user-specified pipeline and outputting an optimal configuration for the problem. It achieves fully automatic pipeline tuning with the combined effort of model selection, data sub-sampling, and hyper-parameter tuning. In this project, new pipeline execution features are added to the existing implementation of ACAI AutoML, improving its performance in both output model performance and execution time. The comparison to auto-sklearn, a state-of-the-art AutoML platform, shows that ACAI AutoML achieves similar, if not better, pipeline tuning performance and supports a much larger variety of pipelines.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 3:40-4:20<br /> <a href="https://youtu.be/vuM_UJ6ZaDE"> Presentation Recording</a></p>
</td>
<td>
<h2>AI Presentation Coach</h2>
<img src="https://i.postimg.cc/C100JBZL/image-Venny-Ayudiani.png" alt="Venny" />
<p>Huiyi Zhang, Venny Ayudiani</p>
<p>Presentation is one of the most common methods to convey ideas and share information, and presentation skills can be improved by training and evaluation. AI Presentation coach is an end-to-end AI-based system that automatically evaluates the effectiveness of the presentation and provides actionable feedback for the presenters to improve their skills. AI Presentation Coach aims to extract features from each constituent of a presentation, use multimodal machine learning to evaluate the presenter's performance across different presentation aspects, and provide actionable feedback to the presenter.<br /><br /> Monday, December 12<br /> GHC 8102<br /> 4:20-5:00<br /> <a href="https://youtu.be/zmJc9wN88gc"> Presentation Recording</a></p>
</td>
<td>
<h2> Understanding of Molecular and Evolutionary Mechanisms for Protein Temperature Adaptation</h2>
<img src="https://i.postimg.cc/FH0b6ph4/Screen-Shot-2022-11-24-at-12-17-40-PM-Yuting-Deng.png" alt="Deng" />
<p>Yuting Deng</p>
<p>Billions of years of evolution have produced millions of diverse species. Adaptation is driven by changes in molecules whose functions give rise to life. Temperature acting as the driving force for evolutionary processes decides the adaptation of protein stability and activities. To demystify the molecular mechanisms underlying protein and temperature adaptation, this project aims to investigate proteins' thermostability through computational methods. Since a well-annotated functional database of protein is lacking, we curated a large-scale well-annotated functional database across archaea and bacterias, including ~9947 ortholog groups annotated by 40 critical function labels under rigorous quality control. In the future, this dataset would be of great value in dissecting biophysical forces that drive protein evolution through identifying temperature-associated residues and trends in temperature-associated residue properties and interactions.<br /><br /> Tuesday, December 13<br /> GHC 4405<br /> 12:00-12:40<br /> <a href= "https://youtu.be/1Ob5nKZ15NY"> Presentation Recording</a></p>
</td>
</tr>
<tr style="text-align: center;">
<td>
<h2>Information Retrieval: NERQ (Named Entity Recognition for Queries)</h2>
<img src="https://i.postimg.cc/52fWHP77/capstone-thumbnail-Dhruv-Arya.png" alt="Arya" />
<p>Dhruv Arya, Nidhi Dhar, Sarthak Tandon</p>
<p>Named Entity Recognition on web search queries (NERQ) has been noted as a challenging problem in the literature. The reduction in context, irregular grammar, and lack of proper casing in queries makes it difficult for off-the-shelf NER models to perform well on queries. This paper introduces a new and challenging supervised dataset for NERQ created from the MS MARCO dataset and performs experiments to improve the performance of the current state-of-the-art models on this dataset. These models show significant performance gains in experiments that perform finetuning with domain-transferred data.<br /><br /> Tuesday, December 13<br /> GHC 4405 <br /> 12:40-1:20<br /> <a href="https://youtu.be/q-10SDBZSkY"> Presentation Recording</a></p>
</td>
<td>
<h2>Exploring Trust and Strategies in Agent-Human teaming</h2>
<img src="https://i.postimg.cc/TPYFPVWb/Human-Agent-Annie-Johnson.jpg" alt="Annie" />
<p>Annie Johnson</p>
<p>This work delves into evaluating trust in agent-human teaming in the gaming domain. This capstone project combines a Human-Computer Interaction (HCI) component as well as an analytic component. Here we provide a definition of &ldquo;trust&rdquo; relevant to the Overcooked game and perform a study where volunteers play this game and answer survey questions based on their experience. The game-play videos recorded during the study were used to train an Object Recognition model to test the robustness of the video. Self-Organizing Maps were used to cluster the trajectories obtained from the game to identify the factors that influence trust.<br /><br /> Tuesday, December 13<br /> GHC 4405<br /> 1:20-2:00<br /> <a href= "https://youtu.be/KGVpMinu7b0"> Presentation Recording</a></p>
</td>
<td>
<h2>AI2F: Artificial Intelligence Integrated Fires</h2>
<img src="https://i.postimg.cc/2S1D4HNC/AI2F.png" alt="AI2F" />
<p>Simon Knapp, Eric Youn, Rebecca Wilson</p>
<p>The current military planning process is slow and deliberate and requires days or even weeks of dedicated staff personnel. Our solution attempts to solve this problem for artillery planning by applying reinforcement learning to develop agents that can rapidly develop courses of action and wargame various scenarios. On a broader scale, the simulation environment we are currently developing alongside the US Army Engineer Research and Design Center (ERDC) provides researchers with a valuable tool for rapidly developing agents for other aspects of military planning. Our experiments explore the training methods for agents using this framework and how agents learn to apply indirect fires and ammunition selection in it.<br /><br /> Tuesday, December 13<br /> GHC 4405<br /> 2:00-2:40<br /> <a href="https://youtu.be/mYqM_RlELhk"> Presentation Recording</a></p>
</td>
</tr>
<tr>
<td>
<h2 style="text-align: center;">Nudge</h2>
<img style="display: block; margin-left: auto; margin-right: auto;" src="https://i.postimg.cc/8cC1xN96/Screen-Shot-2022-11-29-at-10-32-14-PM-Hermes-Suen.png" alt="Suen" />
<p style="text-align: center;">Hermes Suen</p>
<p style= "text-align: center;">Nudge is a project aimed at creating a platform for individuals, institutions, and organizations to invest in real-life behavior change. It is built on the assumption of memetic desire and social contagion and provides monetary rewards to creators of viral TikToks that are related to the desired behavior change.<br /><br /> Tuesday, December 13<br /> GHC 4405<br /> 2:40-3:20<br /> <a href="https://youtu.be/C8Jp9Jm2RPs"> Presentation Recording</a></p>
</td>
</tr>
</tbody>
</table>
</div>
</div>